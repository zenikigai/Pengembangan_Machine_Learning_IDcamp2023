{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO+UVR5LjMlIUS5MmLPfTRC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zenikigai/Pengembangan_Machine_Learning_IDcamp2023/blob/main/LSTM_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Long Short Term Memory (LSTM)\n",
        "\n",
        "Long Short-Term Memory (LSTM) is a commonly used technique in natural language processing that allows models to understand the meaning of a sentence based on word order.\n",
        "\n",
        "\n",
        "To implement LSTM itself is very easy on Keras. We simply add an LSTM layer and fill in the parameters with the desired amount of output from that layer. An example of using LSTM can be seen in the code below. We can put the LSTM layer after the embedding layer and before the Dense layer."
      ],
      "metadata": {
        "id": "pGOZpaQZoojc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J40v-1y-ojYn"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=5000, output_dim=16),\n",
        "    tf.keras.layers.LSTM(64),\n",
        "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(5, activation=\"softmax\")\n",
        "])"
      ]
    }
  ]
}