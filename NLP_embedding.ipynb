{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkuVDZ27taG0UWibVc+8Cn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zenikigai/Pengembangan_Machine_Learning_IDcamp2023/blob/main/NLP_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Embedding\n",
        "\n",
        "In text classification, we need to do embedding which is key in text classification in Tensorflow. Embedding allows ML models to understand the meaning in each word and group words with similar meanings close together. For example comments on a youtube video, where the words \"interesting\", \"cool\", and \"amazing\" will be grouped close together. This grouping can be achieved by mapping each word into vectors or arrays. Where similar words will have similar vector values.\n",
        "\n",
        "The meaning of a word is obtained from the label of the data. For example, in text labeled negatively there are many words 'boring', and 'ugly'. Then the two words have similar meanings so that their vector values are similar."
      ],
      "metadata": {
        "id": "n3qfjVXJrkjf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wliMdk3NrDWi"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Flatten, Dense"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To implement Embedding on Keras is also very easy. In the sequential model, we just call the Embedding() function and fill in the total parameters of the tokenized word, sentence length, and desired embedding dimension. Since the result of embedding is a 2-dimensional array containing the length of each sentence, and the embedding dimension, we need the flatten() function."
      ],
      "metadata": {
        "id": "DSALdm7QuLUN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Embedding(words_lenght, embedding_dimension, input_length)\n",
        "    Flatten(),\n",
        "    Dense(24, activation=\"relu\"),\n",
        "    Dense(1, activation=\"sigmoid\")\n",
        "])"
      ],
      "metadata": {
        "id": "dF3-d-nFspCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After implementing Embedding on our sequential model, call the compile function. For optimizers, we can use the optimizer we learned earlier. While the loss is adjusted to the class contained in the dataset."
      ],
      "metadata": {
        "id": "gKRnLuR4uXQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss = \"categorical_crossentropy\",\n",
        "              optimizer = \"adam\",\n",
        "              metrics = [\"accuracy\"])"
      ],
      "metadata": {
        "id": "8kyJASQytQIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the fit function, we need the parameters of the padding text, the label of the training data, the number of epochs, and the validation data. It's easy, isn't it."
      ],
      "metadata": {
        "id": "cr2KdxZwuuk4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(padded_train, train_label,\n",
        "          epochs = num_epochs,\n",
        "          validation_data = (padded_test, label_test))"
      ],
      "metadata": {
        "id": "s8zxOV8Su0ks"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}