{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwUliP8AUwj84z1F0wUJcU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zenikigai/Pengembangan_Machine_Learning_IDcamp2023/blob/main/NLP_tokenization_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization Exercise\n",
        "\n",
        "Inthis module, we will learn how to do tokenization and make sequence from our text."
      ],
      "metadata": {
        "id": "Fcfe_pm1iRj0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5Nj4UADxdgdv"
      },
      "outputs": [],
      "source": [
        "# tokenizer library\n",
        "from keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If num_words parameter is set to 15, only 15 words appear most often. The 15 words will be tokenized from all words in the dataset.\n",
        "\n",
        "The oov_token parameter is a parameter that serves to replace words that are not tokenized into certain characters.\n",
        "\n",
        "It's better to replace an unrecognized word with a specific word than to skip it to reduce missing information. This is what can be done by adding OOV parameters."
      ],
      "metadata": {
        "id": "64PeniHqj6N_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make tokenizer objek\n",
        "tokenizer = Tokenizer(num_words = 15, oov_token = \"_\")"
      ],
      "metadata": {
        "id": "lVRHb998jCpT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text to be tokenize and we use to train our model\n",
        "teks = [\"I love coding\",\n",
        "        \"Coding is fun!\",\n",
        "        \"Machine learning is diferent from conventional programming\"]"
      ],
      "metadata": {
        "id": "kViJlJZPklCY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To tokenize, call the fit_on_text() function on the tokenizer object and fill in our text as arguments."
      ],
      "metadata": {
        "id": "FwI9lCytlEvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts(teks)"
      ],
      "metadata": {
        "id": "5pZ784QBlQD9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we'll convert the previously created text into a sequence form using the text_to_sequences function."
      ],
      "metadata": {
        "id": "tVrIZ91ElhX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = tokenizer.texts_to_sequences(teks)"
      ],
      "metadata": {
        "id": "iG2eWAfQlY_F"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the tokenization result, we can call the word_index attribute of the tokenizer object. The word index attribute returns a dictionary in the form of a word as a key and a token or numeric value as a value. Note that punctuation and capital letters are not processed by the tokenizer. For example, the words \"Congratulations!\" and \"CONGRATULATIONS\" will be treated as the same word. The result from the cell below shows that words that are out-of-vocabulary will be given a token worth 1."
      ],
      "metadata": {
        "id": "uORWCPA_mPOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7nLm12emSkq",
        "outputId": "b13af6bb-6ffe-411d-9ab9-b010f07bdcc2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'_': 1, 'coding': 2, 'is': 3, 'i': 4, 'love': 5, 'fun': 6, 'machine': 7, 'learning': 8, 'diferent': 9, 'from': 10, 'conventional': 11, 'programming': 12}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, take a look at the code below. The code output below is an example of using a token to convert a sentence into numeric form. In the example, the words 'learn', 'since', and '2022' are marked with a value of \"1\". This indicates that these words are not present in a previously created dictionary (OOV). Without OOV, words that do not have a token are not included in the sequence. If we use OOV, then every word that does not have a token will be assigned a uniform token. With OOV, the order information of each word in the sentence is not lost."
      ],
      "metadata": {
        "id": "aeBwXKL9np1R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.texts_to_sequences([\"I learn programming since 2022\"]))\n",
        "print(tokenizer.texts_to_sequences([\"I love programming\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96cscL7rmcuZ",
        "outputId": "d90ff87a-2ff3-4cae-a39a-5ced242d0a36"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4, 1, 12, 1, 1]]\n",
            "[[4, 5, 12]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After tokenization, to convert the sentence into the corresponding values can be by using the text_to_sequence() function and enter parameters from our text. When the sequence has been created, all we need to do is padding. Yup, padding is the process of making each sentence in the text have a uniform length. Just like resizing images, so that the resolution of each image is the same. To use padding, we need to call the pad_sequence library first. Then, call the pad_sequence() function and enter the tokenized result sequence as its parameter."
      ],
      "metadata": {
        "id": "kz_rfynkoifb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import pad_sequences\n",
        "sequences_sameLength = pad_sequences(sequences)"
      ],
      "metadata": {
        "id": "UdSnOei5ngPp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After padding, each sequence will have the same length. Padding can do this by adding 0 by default at the beginning of shorter sequences"
      ],
      "metadata": {
        "id": "kM2mgnM4pSMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sequences_sameLength)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9caBoOHpXLA",
        "outputId": "e500a4e3-93fe-456c-906e-b7ded117b642"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0  0  0  0  4  5  2]\n",
            " [ 0  0  0  0  2  3  6]\n",
            " [ 7  8  3  9 10 11 12]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want to change so that 0 is added at the end of the sequence, we can use the padding parameter with the value 'post'. In addition, we can set the maxlen parameter (maximum length of each sequence) with the value we want. If we fill in the value 5, then the length of a sequence will not exceed 5."
      ],
      "metadata": {
        "id": "2-KvFS52pjy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences_sameLength = pad_sequences(sequences,\n",
        "                                     padding = \"post\",\n",
        "                                     maxlen = 5)"
      ],
      "metadata": {
        "id": "nC_obMBMpb2l"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If our text has a length greater than the maxlen parameter value for example 5, then by default the value of the sequence will be taken the last 5 values or only the last 5 words of each sentence (ignoring the previous word). To change this setting and retrieve the first 5 words of each sentence, we can use the truncating parameter and fill in the 'post' value."
      ],
      "metadata": {
        "id": "hzDo-2NzqFth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences_sameLength = pad_sequences(sequences,\n",
        "                                     padding = \"post\",\n",
        "                                     maxlen = 5,\n",
        "                                     truncating = \"post\")"
      ],
      "metadata": {
        "id": "gdoOHPHOp5_f"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}