{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQorZOY3ne11A6Fow/sepQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zenikigai/Pengembangan_Machine_Learning_IDcamp2023/blob/main/transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning\n",
        "\n",
        "this lesson based on this https://github.com/zenikigai/Pengembangan_Machine_Learning_IDcamp2023/blob/main/Real_world_image_processing.ipynb"
      ],
      "metadata": {
        "id": "bSXuumL-ctCx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIG_ZhXccq_d",
        "outputId": "4845f18a-f44a-479f-8289-8fbc88b10dc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-16 19:31:21--  https://github.com/dicodingacademy/assets/raw/main/ml_pengembangan_academy/Chessman-image-dataset.zip\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/dicodingacademy/assets/main/ml_pengembangan_academy/Chessman-image-dataset.zip [following]\n",
            "--2023-12-16 19:31:21--  https://raw.githubusercontent.com/dicodingacademy/assets/main/ml_pengembangan_academy/Chessman-image-dataset.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 60684125 (58M) [application/zip]\n",
            "Saving to: ‘/tmp/Chessman-image-dataset.zip’\n",
            "\n",
            "/tmp/Chessman-image 100%[===================>]  57.87M   177MB/s    in 0.3s    \n",
            "\n",
            "2023-12-16 19:31:22 (177 MB/s) - ‘/tmp/Chessman-image-dataset.zip’ saved [60684125/60684125]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate \\\n",
        " https://github.com/dicodingacademy/assets/raw/main/ml_pengembangan_academy/Chessman-image-dataset.zip \\\n",
        " -O /tmp/Chessman-image-dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using the same augmentation based on previous lesson\n",
        "import os\n",
        "import zipfile\n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "AlluwcrWdQho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_zip = \"/tmp/Chessman-image-dataset.zip\"\n",
        "zip_ref = zipfile.ZipFile(local_zip, \"r\")\n",
        "zip_ref.extractall(\"/tmp\")\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "NaZtYCosdtcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = os.path.join(\"/tmp/Chessman-image-dataset/Chess\")\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   rotation_range = 20,\n",
        "                                   zoom_range = 20,\n",
        "                                   shear_range = 0.2,\n",
        "                                   fill_mode = \"nearest\",\n",
        "                                   validation_split = 0.1)"
      ],
      "metadata": {
        "id": "sHhvgkRYeJCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# divide the dataset to be training data and validation data\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size = (150, 150),\n",
        "    batch_size = 8,\n",
        "    class_mode = \"categorical\",\n",
        "    subset = \"training\"\n",
        ")\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size = (150, 150),\n",
        "    batch_size = 16,\n",
        "    class_mode = \"categorical\",\n",
        "    subset = \"validation\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTAomwJke7b7",
        "outputId": "fd9699a5-d5cf-4fec-b067-01964a70fa56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 499 images belonging to 6 classes.\n",
            "Found 52 images belonging to 6 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "implement \"Transfer Learning\" here. The model we choose as the basic model of transfer learning is ResNet152V2. The ResNet152V2 model has as many as 152 layers and is available in keras library. We can use the features that have been learned by the model for our model to use.\n",
        "\n",
        "To implement transfer learning is very easy as the code below. We only need to add 2 different lines of code. The first layer of our model is the model we use for transfer learning. We simply call the ResNet152V2 class and fill in the parameters as follows:\n",
        "\n",
        "  Weight: For the weight parameter we fill in the value 'imagenet'. This means that we want to use the ResNet152V2 model that has been trained on the imagenet dataset. Imagenet is a giant database containing more than 14 million images.\n",
        "  \n",
        "  Include_top : this parameter is boolean. The point of this parameter is if we want to keep using the last layer / prediction layer from the resnet model. We fill in false because we are using a resnet model to predict the Chessman dataset instead of imagenet.\n",
        "  \n",
        "  Input_tensor : as the name implies this parameter specifies the size of the input."
      ],
      "metadata": {
        "id": "DKyiGvSE7say"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.layers import Input, Flatten, Dense\n",
        "from keras.applications import ResNet50, ResNet152V2\n",
        "from keras.models import Sequential\n",
        "\n",
        "model = Sequential([\n",
        "    ResNet152V2(\n",
        "         weights=\"imagenet\",\n",
        "         include_top=False,\n",
        "         input_tensor=Input(shape=(150, 150, 3))\n",
        "    ),\n",
        "    Flatten(),\n",
        "    Dense(512, activation=\"relu\"),\n",
        "    Dense(256, activation=\"relu\"),\n",
        "    Dense(6, activation=\"softmax\")\n",
        "])\n",
        "model.layers[0].trainable = False"
      ],
      "metadata": {
        "id": "4AIhGPVE-OaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimizer and loss\n",
        "model.compile(optimizer = tf.optimizers.Adam(),\n",
        "              loss=\"categorical_crossentropy\",\n",
        "              metrics = [\"accuracy\"])"
      ],
      "metadata": {
        "id": "Zs8LuSmDAy0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "history = model.fit(train_generator,\n",
        "                    validation_data=validation_generator,\n",
        "                    epochs = 50,\n",
        "                    verbose = 1\n",
        "                    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFIM2_FmBZ0N",
        "outputId": "6d8f80b1-948f-4b65-bf22-0a887e485232"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "63/63 [==============================] - 164s 3s/step - loss: 1.9609 - accuracy: 0.2425 - val_loss: 2.0996 - val_accuracy: 0.2115\n",
            "Epoch 2/50\n",
            "63/63 [==============================] - 168s 3s/step - loss: 1.9602 - accuracy: 0.2445 - val_loss: 1.8603 - val_accuracy: 0.2885\n",
            "Epoch 3/50\n",
            "63/63 [==============================] - 170s 3s/step - loss: 1.9530 - accuracy: 0.2164 - val_loss: 2.0473 - val_accuracy: 0.1538\n",
            "Epoch 4/50\n",
            "63/63 [==============================] - 160s 3s/step - loss: 1.8164 - accuracy: 0.2164 - val_loss: 2.0566 - val_accuracy: 0.1923\n",
            "Epoch 5/50\n",
            "63/63 [==============================] - 170s 3s/step - loss: 1.8579 - accuracy: 0.2425 - val_loss: 1.9678 - val_accuracy: 0.1923\n",
            "Epoch 6/50\n",
            "63/63 [==============================] - 168s 3s/step - loss: 1.8121 - accuracy: 0.2505 - val_loss: 2.0065 - val_accuracy: 0.1154\n",
            "Epoch 7/50\n",
            "63/63 [==============================] - 170s 3s/step - loss: 1.8204 - accuracy: 0.2786 - val_loss: 1.9731 - val_accuracy: 0.2500\n",
            "Epoch 8/50\n",
            "63/63 [==============================] - 160s 3s/step - loss: 1.7989 - accuracy: 0.2585 - val_loss: 1.8738 - val_accuracy: 0.2500\n",
            "Epoch 9/50\n",
            "63/63 [==============================] - 172s 3s/step - loss: 1.7226 - accuracy: 0.2926 - val_loss: 2.0362 - val_accuracy: 0.1731\n",
            "Epoch 10/50\n",
            "63/63 [==============================] - 162s 3s/step - loss: 1.7674 - accuracy: 0.2946 - val_loss: 1.8927 - val_accuracy: 0.2308\n",
            "Epoch 11/50\n",
            "63/63 [==============================] - 172s 3s/step - loss: 1.7315 - accuracy: 0.3086 - val_loss: 1.8840 - val_accuracy: 0.2308\n",
            "Epoch 12/50\n",
            "63/63 [==============================] - 168s 3s/step - loss: 1.7913 - accuracy: 0.3026 - val_loss: 2.2097 - val_accuracy: 0.2500\n",
            "Epoch 13/50\n",
            "63/63 [==============================] - 173s 3s/step - loss: 1.7612 - accuracy: 0.2665 - val_loss: 2.1016 - val_accuracy: 0.1731\n",
            "Epoch 14/50\n",
            "63/63 [==============================] - 170s 3s/step - loss: 1.7487 - accuracy: 0.2806 - val_loss: 2.0341 - val_accuracy: 0.1538\n",
            "Epoch 15/50\n",
            "63/63 [==============================] - 172s 3s/step - loss: 1.6928 - accuracy: 0.2826 - val_loss: 1.9974 - val_accuracy: 0.1538\n",
            "Epoch 16/50\n",
            "63/63 [==============================] - 168s 3s/step - loss: 1.6632 - accuracy: 0.3086 - val_loss: 2.1773 - val_accuracy: 0.1538\n",
            "Epoch 17/50\n",
            "63/63 [==============================] - 169s 3s/step - loss: 1.7316 - accuracy: 0.3066 - val_loss: 2.0653 - val_accuracy: 0.0962\n",
            "Epoch 18/50\n",
            "63/63 [==============================] - 162s 3s/step - loss: 1.6782 - accuracy: 0.2745 - val_loss: 2.0769 - val_accuracy: 0.0962\n",
            "Epoch 19/50\n",
            "63/63 [==============================] - 160s 3s/step - loss: 1.6714 - accuracy: 0.2946 - val_loss: 2.0369 - val_accuracy: 0.1538\n",
            "Epoch 20/50\n",
            "63/63 [==============================] - 164s 3s/step - loss: 1.6902 - accuracy: 0.2866 - val_loss: 1.9022 - val_accuracy: 0.1538\n",
            "Epoch 21/50\n",
            "63/63 [==============================] - 160s 3s/step - loss: 1.7292 - accuracy: 0.2786 - val_loss: 1.9787 - val_accuracy: 0.2115\n",
            "Epoch 22/50\n",
            "63/63 [==============================] - 182s 3s/step - loss: 1.6288 - accuracy: 0.3066 - val_loss: 1.9956 - val_accuracy: 0.2500\n",
            "Epoch 23/50\n",
            "63/63 [==============================] - 185s 3s/step - loss: 1.6475 - accuracy: 0.3166 - val_loss: 1.9935 - val_accuracy: 0.1923\n",
            "Epoch 24/50\n",
            "63/63 [==============================] - 182s 3s/step - loss: 1.6844 - accuracy: 0.2886 - val_loss: 1.9161 - val_accuracy: 0.2308\n",
            "Epoch 25/50\n",
            "63/63 [==============================] - 170s 3s/step - loss: 1.6318 - accuracy: 0.3026 - val_loss: 2.1662 - val_accuracy: 0.1154\n",
            "Epoch 26/50\n",
            "63/63 [==============================] - 163s 3s/step - loss: 1.6747 - accuracy: 0.2806 - val_loss: 2.0473 - val_accuracy: 0.1923\n",
            "Epoch 27/50\n",
            "63/63 [==============================] - 160s 3s/step - loss: 1.7211 - accuracy: 0.3267 - val_loss: 2.0679 - val_accuracy: 0.2115\n",
            "Epoch 28/50\n",
            "63/63 [==============================] - 169s 3s/step - loss: 1.6518 - accuracy: 0.3126 - val_loss: 2.0594 - val_accuracy: 0.2308\n",
            "Epoch 29/50\n",
            "63/63 [==============================] - 172s 3s/step - loss: 1.6528 - accuracy: 0.3287 - val_loss: 2.0702 - val_accuracy: 0.1346\n",
            "Epoch 30/50\n",
            "63/63 [==============================] - 168s 3s/step - loss: 1.6352 - accuracy: 0.3267 - val_loss: 2.0122 - val_accuracy: 0.1923\n",
            "Epoch 31/50\n",
            "63/63 [==============================] - 164s 3s/step - loss: 1.6531 - accuracy: 0.3287 - val_loss: 1.9722 - val_accuracy: 0.1731\n",
            "Epoch 32/50\n",
            "63/63 [==============================] - 161s 3s/step - loss: 1.6334 - accuracy: 0.3307 - val_loss: 1.9204 - val_accuracy: 0.1731\n",
            "Epoch 33/50\n",
            "63/63 [==============================] - 171s 3s/step - loss: 1.6176 - accuracy: 0.3186 - val_loss: 2.0187 - val_accuracy: 0.1538\n",
            "Epoch 34/50\n",
            "63/63 [==============================] - 170s 3s/step - loss: 1.6153 - accuracy: 0.3307 - val_loss: 2.0241 - val_accuracy: 0.0962\n",
            "Epoch 35/50\n",
            "63/63 [==============================] - 173s 3s/step - loss: 1.6457 - accuracy: 0.3307 - val_loss: 1.9599 - val_accuracy: 0.2115\n",
            "Epoch 36/50\n",
            "63/63 [==============================] - 162s 3s/step - loss: 1.5741 - accuracy: 0.3727 - val_loss: 1.8743 - val_accuracy: 0.2115\n",
            "Epoch 37/50\n",
            "63/63 [==============================] - 170s 3s/step - loss: 1.5878 - accuracy: 0.3768 - val_loss: 2.0821 - val_accuracy: 0.1346\n",
            "Epoch 38/50\n",
            "63/63 [==============================] - 171s 3s/step - loss: 1.5591 - accuracy: 0.3407 - val_loss: 2.1284 - val_accuracy: 0.1731\n",
            "Epoch 39/50\n",
            "63/63 [==============================] - 170s 3s/step - loss: 1.6087 - accuracy: 0.3347 - val_loss: 2.1051 - val_accuracy: 0.1731\n",
            "Epoch 40/50\n",
            "63/63 [==============================] - 172s 3s/step - loss: 1.5593 - accuracy: 0.3848 - val_loss: 2.1303 - val_accuracy: 0.1923\n",
            "Epoch 41/50\n",
            "63/63 [==============================] - 170s 3s/step - loss: 1.5888 - accuracy: 0.3507 - val_loss: 1.9498 - val_accuracy: 0.2500\n",
            "Epoch 42/50\n",
            "63/63 [==============================] - 168s 3s/step - loss: 1.5505 - accuracy: 0.3627 - val_loss: 1.8484 - val_accuracy: 0.2308\n",
            "Epoch 43/50\n",
            "63/63 [==============================] - 172s 3s/step - loss: 1.5587 - accuracy: 0.3607 - val_loss: 2.0698 - val_accuracy: 0.0962\n",
            "Epoch 44/50\n",
            "63/63 [==============================] - 168s 3s/step - loss: 1.6218 - accuracy: 0.3447 - val_loss: 1.9542 - val_accuracy: 0.1538\n",
            "Epoch 45/50\n",
            "63/63 [==============================] - 161s 3s/step - loss: 1.5717 - accuracy: 0.3407 - val_loss: 1.9091 - val_accuracy: 0.2115\n",
            "Epoch 46/50\n",
            "63/63 [==============================] - 173s 3s/step - loss: 1.5937 - accuracy: 0.3487 - val_loss: 1.8832 - val_accuracy: 0.2885\n",
            "Epoch 47/50\n",
            "63/63 [==============================] - 171s 3s/step - loss: 1.5272 - accuracy: 0.3667 - val_loss: 2.0402 - val_accuracy: 0.2115\n",
            "Epoch 48/50\n",
            "63/63 [==============================] - 173s 3s/step - loss: 1.5433 - accuracy: 0.3707 - val_loss: 1.9688 - val_accuracy: 0.2500\n",
            "Epoch 49/50\n",
            "63/63 [==============================] - 164s 3s/step - loss: 1.5740 - accuracy: 0.3687 - val_loss: 2.1559 - val_accuracy: 0.2115\n",
            "Epoch 50/50\n",
            "63/63 [==============================] - 162s 3s/step - loss: 1.5586 - accuracy: 0.3667 - val_loss: 2.0294 - val_accuracy: 0.2885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WOmHgUUOBusG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}